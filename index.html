<!doctype html>
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FT8PM5GF04"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-FT8PM5GF04');
</script>

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="I am a PhD candidate in the Department of Computer Science and Applied Mathematics at the Weizmann Institute of Science under the supervision of Prof. Yaron Lipman. My research focuses on developing deep learning models for iregular data, mainly graphs and point clouds. ">
<title>Omri Puny </title>
<link href="AboutPageAssets/styles/aboutPageStyle.css" rel="stylesheet" type="text/css">
<style type="text/css">
</style>

<!--The following script tag downloads a font from the Adobe Edge Web Fonts server for use within the web page. We recommend that you do not modify it.-->
<script>var __adobewebfontsappname__="dreamweaver"</script><script src="https://use.edgefonts.net/montserrat:n4:default;source-sans-pro:n2:default.js" type="text/javascript"></script>
</head>

<body alink = "#282727" vlink = "#9b9b9b" link = "#9b9b9b">
<!-- Header content -->
<header>
  <div class="profilePhoto"> 
    <!-- Profile photo --> 
    <img src="AboutPageAssets/images/omri_image.jpeg" alt="sample" width="250"> </div>
  <!-- Identity details -->
  <section class="profileHeader">
    <h1>Omri Puny</h1>
    <h3>PhD Candidate</h3>
    <hr>
    <p>I am a PhD student in the Department of Computer Science and Applied Mathematics at the Weizmann Institute of Science under the supervision of <a href="https://www.wisdom.weizmann.ac.il/~ylipman/" target="_blank"> Prof. Yaron Lipman</a>. My research focuses on developing deep learning models for iregular data, mainly graphs and point clouds. </p>
    <p>&nbsp;</p>
    <p>&nbsp;</p>
  </section>
  <!-- Links to Social network accounts -->
  <aside class="socialNetworkNavBar">
	   <div class="socialNetworkNav"> 
      <!-- Add a Anchor tag with nested img tag here --> 
		   <a href="mailto:omri.puny@weizmann.ac.il">
      <img src="AboutPageAssets/images/mail.png"  alt="sample" width="30" ></a> </div>
    <div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://github.com/omri1348" target="_blank">
      <img src="AboutPageAssets/images/github.png" alt="sample" width="30"></a>  </div>
    <div class="socialNetworkNav"> 
		<a href="		https://scholar.google.com/citations?user=7GXWu34AAAAJ&hl=en
" target="_blank">
      <!-- Add a Anchor tag with nested img tag here --> 
      <img src="AboutPageAssets/images/scholar.jpg"  alt="sample" width="30"></a>  </div>
	  <div class="socialNetworkNav">
		 <a href="https://twitter.com/OmriPuny" target="_blank">
			 <img src="AboutPageAssets/images/twit.png" alt="sample" width="30"> </a></div>
	  
  </aside>
</header>
<!-- content -->
<section class="mainContent"> 
  <!-- Contact details -->
  
  <!-- Previous experience details -->
  
  
  <section class="section2">
  <h2 class="sectionTitle">Publications</h2>
    <hr class="sectionTitleRule">
    <hr class="sectionTitleRule2">
	  

	   <div class="sectionContent">
	  <img src="AboutPageAssets/images/dflow.png" style='height: 100%; width: 100%; object-fit: contain' alt=""/>
	   </div>
     <section class="section2Content">
      <h2 class="sectionContentTitle">  D-Flow: Differentiating through Flows for Controlled Generation
       </h2>
      <h3 class="sectionContentSubTitle">Heli Ben-Hamu, <strong>Omri Puny</strong>, Itai Gat, Brian Karrer, Uriel Singer, Yaron Lipman 
 </h3>
      <h3 class="sectionContentSubTitle"><em>International Conference on Machine Learning (ICML 2024) 

</em></h3>
      <!-- Second Title & company details  -->

    
    </section>
     <aside class="externalResourcesNav"> 
      <div class="dropdown"> <span>Abstract</span>
        <div class="dropdown-content">
<p>Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all.</p>

        </div>
      </div>
      </div>
	  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2402.14017" target="_blank">Arxiv</a></div>

    </aside>
    
	   <div class="sectionContent">
	  <img src="AboutPageAssets/images/msdf.jpg" style='height: 100%; width: 100%; object-fit: contain' alt=""/>
	   </div>
     <section class="section2Content">
      <h2 class="sectionContentTitle">  Mosaic-SDF for 3D Generative Models       </h2>
      <h3 class="sectionContentSubTitle">Lior Yariv, <strong>Omri Puny</strong>, Natalia Neverova, Oran Gafni, Yaron Lipman
 </h3>
      <h3 class="sectionContentSubTitle"><em>Conference on Computer Vision and Pattern Recognition (CVPR) 2024
</em></h3>
      <!-- Second Title & company details  -->

    
    </section>
     <aside class="externalResourcesNav"> 
      <div class="dropdown"> <span>Abstract</span>
        <div class="dropdown-content">
          <p>Current diffusion or flow-based generative models for 3D shapes divide to two: distilling pre-trained 2D image diffusion models, and training directly on 3D shapes. When training a diffusion or flow models on 3D shapes a crucial design choice is the shape representation. An effective shape representation needs to adhere three design principles: it should allow an efficient conversion of large 3D datasets to the representation form; it should provide a good tradeoff of approximation power versus number of parameters; and it should have a simple tensorial form that is compatible with existing powerful neural architectures. While standard 3D shape representations such as volumetric grids and point clouds do not adhere to all these principles simultaneously, we advocate in this paper a new representation that does. We introduce Mosaic-SDF (M-SDF): a simple 3D shape representation that approximates the Signed Distance Function (SDF) of a given shape by using a set of local grids spread near the shape's boundary. The M-SDF representation is fast to compute for each shape individually making it readily parallelizable; it is parameter efficient as it only covers the space around the shape's boundary; and it has a simple matrix form, compatible with Transformer-based architectures. We demonstrate the efficacy of the M-SDF representation by using it to train a 3D generative flow model including class-conditioned generation with the 3D Warehouse dataset, and text-to-3D generation using a dataset of about 600k caption-shape pairs.
        </div>
      </div>
      </div>
	  <div class="dropdown"><span></span><a href="https://arxiv.org/html/2312.09222v2" target="_blank">Arxiv</a></div>
    <div class="dropdown"><span></span><a href="https://lioryariv.github.io/msdf/" target="_blank">Project Page</a></div>
    </aside>


	   <div class="sectionContent">
	  <img src="AboutPageAssets/images/equi_poly.png" style='height: 100%; width: 100%; object-fit: contain' alt=""/>
	   </div>
     <section class="section2Content">
      <h2 class="sectionContentTitle">  Equivariant Polynomials for Graph Neural Networks
       </h2>
      <h3 class="sectionContentSubTitle"><strong>Omri Puny*</strong>, Derek Lim*, Bobak T. Kiani*, Haggai Maron, Yaron Lipman (*equal contribution)
 </h3>
      <h3 class="sectionContentSubTitle"><em>International Conference on Machine Learning (ICML 2023) <strong>Oral</strong> 

</em></h3>
      <!-- Second Title & company details  -->

    
    </section>
     <aside class="externalResourcesNav"> 
      <div class="dropdown"> <span>Abstract</span>
        <div class="dropdown-content">
          <p>Graph Neural Networks (GNN) are inherently limited in their expressive power. Recent seminal works (Xu et al., 2019; Morris et al., 2019b) introduced the Weisfeiler-Lehman (WL) hierarchy as a measure of expressive power. Although this hierarchy has propelled significant advances in GNN analysis and architecture developments, it suffers from several significant limitations. These include a complex definition that lacks direct guidance for model improvement and a WL hierarchy that is too coarse to study current GNNs. This paper introduces an alternative expressive power hierarchy based on the ability of GNNs to calculate equivariant polynomials of a certain degree. As a first step, we provide a full characterization of all equivariant graph polynomials by introducing a concrete basis, significantly generalizing previous results. Each basis element corresponds to a specific multi-graph, and its computation over some graph data input corresponds to a tensor contraction problem. Second, we propose algorithmic tools for evaluating the expressiveness of GNNs using tensor contraction sequences, and calculate the expressive power of popular GNNs. Finally, we enhance the expressivity of common GNN architectures by adding polynomial features or additional operations / aggregations inspired by our theory. These enhanced GNNs demonstrate state-of-the-art results in experiments across multiple graph learning benchmarks.
</p>
        </div>
      </div>
      </div>
	  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2302.11556" target="_blank">Arxiv</a></div>
    <div class="dropdown"><span></span><a href="https://github.com/omri1348/Equivariant-Polynomaials" target="_blank">Github</a></div>
    </aside>
    

	    
	   <div class="sectionContent">
	  <img src="AboutPageAssets/images/frames.png" style='height: 100%; width: 100%; object-fit: contain' alt=""/>
	   </div>
     <section class="section2Content">
      <h2 class="sectionContentTitle"> Frame Averaging for Invariant and Equivariant Network Design

       </h2>
      <h3 class="sectionContentSubTitle"><strong>Omri Puny*</strong>, Matan Atzmon*, Heli Ben-Hamu*, Edward J. Smith, Ishan Misra, Aditya Grover, Yaron Lipman (*equal contribution)
 </h3>
      <h3 class="sectionContentSubTitle"><em>International Conference on Learning Representations (ICLR 2022) <strong>Oral</strong>

</em></h3>
      <!-- Second Title & company details  -->

    
    </section>
     <aside class="externalResourcesNav"> 
      <div class="dropdown"> <span>Abstract</span>
        <div class="dropdown-content">
          <p>Many machine learning tasks involve learning functions that are known to be invariant or equivariant to certain symmetries of the input data. However, it is often challenging to design neural network architectures that respect these symmetries while being expressive and computationally efficient. For example, Euclidean motion invariant/equivariant graph or point cloud neural networks. We introduce Frame Averaging (FA), a general purpose and systematic framework for adapting known (backbone) architectures to become invariant or equivariant to new symmetry types. Our framework builds on the well known group averaging operator that guarantees invariance or equivariance but is intractable. In contrast, we observe that for many important classes of symmetries, this operator can be replaced with an averaging operator over a small subset of the group elements, called a frame. We show that averaging over a frame guarantees exact invariance or equivariance while often being much simpler to compute than averaging over the entire group. Furthermore, we prove that FA-based models have maximal expressive power in a broad setting and in general preserve the expressive power of their backbone architectures. Using frame averaging, we propose a new class of universal Graph Neural Networks (GNNs), universal Euclidean motion invariant point cloud networks, and Euclidean motion invariant Message Passing (MP) GNNs. We demonstrate the practical effectiveness of FA on several applications including point cloud normal estimation, beyond 2-WL graph separation, and n-body dynamics prediction, achieving state-of-the-art results in all of these benchmarks.
</p>
        </div>
      </div>
      </div>
	  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2110.03336" target="_blank">Arxiv</a></div>
    <div class="dropdown"><span></span><a href="https://github.com/omri1348/Frame-Averaging" target="_blank">Github</a></div>
    </aside>
	
	  
	  
    <!-- First Title & company details  -->
	  
	   <div class="sectionContent">
      <img src="AboutPageAssets/images/LRGA4.png" style='height: 100%; width: 100%; object-fit: contain' alt=""/>

    </div>
     <section class="section2Content">
      <h2 class="sectionContentTitle"> Global Attention Improves Graph Networks Generalization
 </h2>
      <h3 class="sectionContentSubTitle"><strong>Omri Puny</strong>, Heli Ben-Hamu, Yaron Lipman </h3>
      <h3 class="sectionContentSubTitle"><em>Graph Representation Learning and Beyond Workshop (ICML) 2020</em></h3>
      <!-- Second Title & company details  -->

    
    </section>
	  <aside class="externalResourcesNav"> 
      <div class="dropdown"> <span>Abstract</span>
        <div class="dropdown-content">
          <p>This paper advocates incorporating a Low-Rank Global Attention (LRGA) module, a computation and memory efficient variant of the dot-product attention (Vaswani et al., 2017), to Graph Neural Networks (GNNs) for improving their generalization power. To theoretically quantify the generalization properties granted by adding the LRGA module to GNNs, we focus on a specific family of expressive GNNs and show that augmenting it with LRGA provides algorithmic alignment to a powerful graph isomorphism test, namely the 2-Folklore Weisfeiler-Lehman (2-FWL) algorithm. In more detail we: (i) consider the recent Random Graph Neural Network (RGNN) (Sato et al., 2020) framework and prove that it is universal in probability; (ii) show that RGNN augmented with LRGA aligns with 2-FWL update step via polynomial kernels; and (iii) bound the sample complexity of the kernel's feature map when learned with a randomly initialized two-layer MLP. From a practical point of view, augmenting existing GNN layers with LRGA produces state of the art results in current GNN benchmarks. Lastly, we observe that augmenting various GNN architectures with LRGA often closes the performance gap between different models.
</p>
        </div>
      </div>
      </div>
	  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2006.07846" target="_blank">Arxiv</a></div>
	  <div class="dropdown"><span></span><a href="https://youtu.be/ZDAyG48B-LA" target="_blank">YouTube</a></div>
	  <div class="dropdown"><span></span><a href="https://github.com/omri1348/LRGA" target="_blank">Github</a></div>
    </aside>
	
	  
 	

    <!-- Second Title & company details  -->


	
	
	


<!-- Replicate the above Div block to add more title and company details --> 
</section>

  <hr>
  
</section>
<footer>
  <p class="footerDisclaimer"> <span></span></p>
  <p class="footerNote"></p>
</footer>
</body>
</html>
